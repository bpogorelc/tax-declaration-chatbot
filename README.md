# ğŸ‡©ğŸ‡ª German Tax Declaration RAG-based Chatbot

A [Streamlit](https://streamlit.io/) app that answers questions about filing a **SteuererklÃ¤rung** (German tax declaration) using only the content of [HandbookGermany.de â€“ Tax Declaration](https://handbookgermany.de/en/tax-declaration).

Powered by **Retrievalâ€‘Augmented Generation (RAG)** with [LangChain](https://python.langchain.com/), a **FAISS** vector database, and your choice of:

- **OpenAI** chat models (GPTâ€‘3.5/4) â€“ _or_
- **Localâ€¯Llamaâ€‘cpp** models (LlamaÂ 2, Mistral, Phiâ€‘2 â€¦) â€“ fully offline.

| Feature                                     | âœ”ï¸  |
| ------------------------------------------- | --- |
| Oneâ€‘click scrape & index of the target page | âœ…  |
| Conversational memory                       | âœ…  |
| Model swap via environment var              | âœ…  |
| Persistent FAISS index                      | âœ…  |
| Runs on CPU or GPU                          | âœ…  |

---

## ğŸ¥ Video demo

This is a screenshot of an actual answer of the app:

```markdown
![Chatbot walkthrough](docs/demo_tax_declaration_chatbot.gif)
```

## ğŸ“¸ Demo

This is a screenshot of an actual answer of the app:

![App screenshot showing the chatbot answering a German taxâ€‘declaration question](docs/screenshot.png)

```
streamlit run tax_declaration_chatbot.py
```

---

## ğŸš€ QuickÂ start (OpenAI backend)

```bash
# 1Â Clone
 git clone https://github.com/yourname/tax-declaration-chatbot.git
 cd tax-declaration-chatbot

# 2Â Create venv & install deps
 python -m venv venv && source venv/bin/activate    # Windows: venv\Scripts\activate
 pip install -r requirements.txt

# 3Â Add your OpenAI key
 export OPENAI_API_KEY="sk-â€¦"                     # WindowsÂ PowerShell: $Env:OPENAI_API_KEY = "sk-â€¦"

# 4Â Run
 streamlit run tax_declaration_chatbot.py
```

The first question triggers a webâ€‘scrape and builds `data/faiss_index/` (few seconds). Subsequent runs load the index instantly.

---

## âš¡ QuickÂ start (localâ€¯Llamaâ€‘cpp backend)

```bash
# 1Â Install llamaâ€‘cpp Python wheel (CPUâ€‘only example)
pip install llama-cpp-python==0.2.34 \
    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu

# 2Â Download a quantised GGUF weight (~6Â GB RAM)
mkdir -p models
wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf \
     -O models/llama-2-7b-chat.gguf

# 3Â Point the app to the model
export LLAMA_CPP_MODEL="$(pwd)/models/llama-2-7b-chat.gguf"   # Windows: setx LLAMA_CPP_MODEL C:\path\to\â€¦

# 4Â (Oneâ€‘time) rebuild the vector store
rm -rf data/faiss_index

# 5Â Run
streamlit run tax_declaration_chatbot.py
```

Leave `OPENAI_API_KEY` **unset** and the script automatically uses the local model.

---

## ğŸ› ï¸ DetailedÂ setup

### 1Â Requirements

- PythonÂ â‰¥Â 3.9
- `pip`, `virtualenv`/`venv`
- 2Â GBÂ disk for repo + index + model
- **OpenAI plan** _or_ \~6â€“8Â GBÂ RAM for a 7â€‘B Llama quant

### 2Â Install dependencies

```bash
pip install -r requirements.txt          # base deps (Streamlit, LangChain, FAISS, requestsâ€¦)
# OptionalÂ â€“ local models
pip install llama-cpp-python             # CPU build
# Or GPU build: see README section above
```

### 3Â Environment variables

| Variable          | When required       | Example                           |
| ----------------- | ------------------- | --------------------------------- |
| `OPENAI_API_KEY`  | Using OpenAI models | `sk-â€¦`                            |
| `LLAMA_CPP_MODEL` | Using llamaâ€‘cpp     | `/full/path/llama-2-7b-chat.gguf` |

### 4Â Vector store

- Stored at \`\`.
- To force a fresh scrape/index run: `rm -rf data/faiss_index`.

### 5Â Run / Develop

```bash
streamlit run tax_declaration_chatbot.py --server.headless false
```

Hotâ€‘reload kicks in when you edit the file.

### 6Â Docker (optional)

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY . /app
RUN pip install -r requirements.txt \
    && pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu
ENV LLAMA_CPP_MODEL=/app/models/llama-2-7b-chat.gguf
EXPOSE 8501
CMD ["streamlit", "run", "tax_declaration_chatbot.py", "--server.port", "8501", "--server.address", "0.0.0.0"]
```

---

## ğŸ“‚ ProjectÂ structure

```
.
â”œâ”€â”€ tax_declaration_chatbot.py   # main Streamlit + RAG script
â”œâ”€â”€ requirements.txt            # pip deps
â”œâ”€â”€ README.md
â”œâ”€â”€ data/                       # scraped text + FAISS index (autoâ€‘created)
â””â”€â”€ models/                      # GGUF weights (if using llamaâ€‘cpp)
```

---

## ğŸ¤ Contributing

1. Fork & create a feature branch.
2. Run `black . && isort .` before committing.
3. Open a PRâ€”describe your change and reference an issue if applicable.

---

## ğŸ“œ License

[MIT](LICENSE) â€“ free for personal & commercial use. The HandbookGermany page content is used under its original license; check their site for details.

---

## ğŸ™ Credits

- [Handbook Germany](https://handbookgermany.de/) for the excellent taxâ€‘declaration guide.
- [LangChain](https://github.com/langchain-ai/langchain) + [FAISS](https://github.com/facebookresearch/faiss) for the RAG plumbing.
- [TheBloke](https://huggingface.co/TheBloke) for maintaining community GGUF quantisations.
